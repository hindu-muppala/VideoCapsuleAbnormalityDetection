{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10779893,"sourceType":"datasetVersion","datasetId":6688727},{"sourceId":10841834,"sourceType":"datasetVersion","datasetId":6733049}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchvision.models as models\nimport torch\nfrom torchvision.transforms import v2\nimport torchvision.transforms as transforms\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\nimport torchvision.datasets as datasets\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn as nn\nimport random\nimport numpy as np\nfrom sklearn.metrics import f1_score, balanced_accuracy_score, roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:03:48.777255Z","iopub.execute_input":"2025-03-27T15:03:48.777530Z","iopub.status.idle":"2025-03-27T15:03:55.893772Z","shell.execute_reply.started":"2025-03-27T15:03:48.777497Z","shell.execute_reply":"2025-03-27T15:03:55.893019Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**Augumentation**","metadata":{}},{"cell_type":"code","source":"minority_augmentation = transforms.Compose([\n    \n    transforms.Resize((224, 224)),\n\n    transforms.RandomRotation(degrees=(0, 10)), \n\n    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.5),\n\n    transforms.RandomApply([transforms.RandomVerticalFlip()], p=0.5),\n\n    transforms.ToTensor()\n    \n])\n\n\nmajority_augmentation = transforms.Compose([\n\n    transforms.Resize((224, 224)),\n    \n    transforms.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:16.749960Z","iopub.execute_input":"2025-03-27T15:04:16.750291Z","iopub.status.idle":"2025-03-27T15:04:16.755642Z","shell.execute_reply.started":"2025-03-27T15:04:16.750268Z","shell.execute_reply":"2025-03-27T15:04:16.754624Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ResNetTransformerClassifier(nn.Module):\n    \n    def __init__(self, num_classes=4, d_model=512, num_heads=8, num_layers=1):\n        \n        super(ResNetTransformerClassifier, self).__init__()\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  \n        self.resnet.fc = nn.Linear(2048, d_model)  \n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Classification Layer\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        \n        x = self.resnet(x)  \n        x = x.unsqueeze(1)  \n        \n        x = self.transformer(x)  # Pass through transformer\n        x = x.squeeze(1)  # Remove sequence dimension\n        \n        return self.fc(x) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:19.827367Z","iopub.execute_input":"2025-03-27T15:04:19.827677Z","iopub.status.idle":"2025-03-27T15:04:19.834120Z","shell.execute_reply.started":"2025-03-27T15:04:19.827653Z","shell.execute_reply":"2025-03-27T15:04:19.833021Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset_path = '/kaggle/input/trining-dataset/training'\nvalidationDataset_path = '/kaggle/input/validation/validating'\n\ntraining_dataset = datasets.ImageFolder(root=dataset_path)\nvalidation_dataset = datasets.ImageFolder(root=validationDataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:22.551866Z","iopub.execute_input":"2025-03-27T15:04:22.552227Z","iopub.status.idle":"2025-03-27T15:05:49.172235Z","shell.execute_reply.started":"2025-03-27T15:04:22.552196Z","shell.execute_reply":"2025-03-27T15:05:49.171194Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Loading the sample**","metadata":{}},{"cell_type":"code","source":"targets = np.array([label for _, label in training_dataset.samples])\n\nclass_counts = np.bincount(targets)\n\nclass_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:21.524799Z","iopub.execute_input":"2025-03-27T15:06:21.525171Z","iopub.status.idle":"2025-03-27T15:06:21.579314Z","shell.execute_reply.started":"2025-03-27T15:06:21.525139Z","shell.execute_reply":"2025-03-27T15:06:21.578075Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"targets, class_counts, class_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:23.919400Z","iopub.execute_input":"2025-03-27T15:06:23.919725Z","iopub.status.idle":"2025-03-27T15:06:23.970807Z","shell.execute_reply.started":"2025-03-27T15:06:23.919701Z","shell.execute_reply":"2025-03-27T15:06:23.969856Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(array([0, 0, 0, ..., 3, 3, 3]),\n array([ 1154,  2694, 28663,  1162]),\n tensor([8.6655e-04, 3.7120e-04, 3.4888e-05, 8.6059e-04]))"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"minority_classes = [0, 1, 3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:26.763747Z","iopub.execute_input":"2025-03-27T15:06:26.764165Z","iopub.status.idle":"2025-03-27T15:06:26.767812Z","shell.execute_reply.started":"2025-03-27T15:06:26.764137Z","shell.execute_reply":"2025-03-27T15:06:26.767020Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**loading the dataset**","metadata":{}},{"cell_type":"code","source":"dataset_path = \"./trining-dataset/training\"  \n\nclass TrainingDataset(Dataset):\n    \n    def __init__(self, dataset, minority_classes):\n        self.dataset = dataset\n        self.minority_classes = set(minority_classes) \n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image_path, label = self.dataset.samples[idx]\n        image = self.dataset.loader(image_path) \n\n        if label in self.minority_classes:\n            image = minority_augmentation(image)\n        else:\n            image = majority_augmentation(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:29.883316Z","iopub.execute_input":"2025-03-27T15:06:29.883643Z","iopub.status.idle":"2025-03-27T15:06:29.889126Z","shell.execute_reply.started":"2025-03-27T15:06:29.883617Z","shell.execute_reply":"2025-03-27T15:06:29.887976Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"sample_weights = [class_weights[label] for label in targets]\n\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:33.034227Z","iopub.execute_input":"2025-03-27T15:06:33.034566Z","iopub.status.idle":"2025-03-27T15:06:33.300670Z","shell.execute_reply.started":"2025-03-27T15:06:33.034533Z","shell.execute_reply":"2025-03-27T15:06:33.299944Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_dataset = TrainingDataset(training_dataset, minority_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:35.536998Z","iopub.execute_input":"2025-03-27T15:06:35.537317Z","iopub.status.idle":"2025-03-27T15:06:35.541161Z","shell.execute_reply.started":"2025-03-27T15:06:35.537294Z","shell.execute_reply":"2025-03-27T15:06:35.540146Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:41.740577Z","iopub.execute_input":"2025-03-27T15:06:41.740978Z","iopub.status.idle":"2025-03-27T15:06:41.745094Z","shell.execute_reply.started":"2025-03-27T15:06:41.740946Z","shell.execute_reply":"2025-03-27T15:06:41.744121Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class ValidationDataset(Dataset):\n    \n    def __init__(self, dataset, minority_classes):\n        self.dataset = dataset\n        self.minority_classes = set(minority_classes) \n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image_path, label = self.dataset.samples[idx]\n        image = self.dataset.loader(image_path) \n\n        if label in self.minority_classes:\n            image = minority_augmentation(image)\n        else:\n            image = majority_augmentation(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:47.229119Z","iopub.execute_input":"2025-03-27T15:06:47.229409Z","iopub.status.idle":"2025-03-27T15:06:47.234690Z","shell.execute_reply.started":"2025-03-27T15:06:47.229387Z","shell.execute_reply":"2025-03-27T15:06:47.233707Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**loading validation dataset**","metadata":{}},{"cell_type":"code","source":"validation_dataset2 = ValidationDataset(validation_dataset,  minority_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:50.289994Z","iopub.execute_input":"2025-03-27T15:06:50.290308Z","iopub.status.idle":"2025-03-27T15:06:50.294000Z","shell.execute_reply.started":"2025-03-27T15:06:50.290283Z","shell.execute_reply":"2025-03-27T15:06:50.293107Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"valid_loader = DataLoader( validation_dataset2, batch_size=128,  shuffle=False,\n                        \n    drop_last=False  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:53.111558Z","iopub.execute_input":"2025-03-27T15:06:53.111924Z","iopub.status.idle":"2025-03-27T15:06:53.116005Z","shell.execute_reply.started":"2025-03-27T15:06:53.111892Z","shell.execute_reply":"2025-03-27T15:06:53.114967Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = ResNetTransformerClassifier()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:06:55.815546Z","iopub.execute_input":"2025-03-27T15:06:55.815924Z","iopub.status.idle":"2025-03-27T15:06:57.362682Z","shell.execute_reply.started":"2025-03-27T15:06:55.815896Z","shell.execute_reply":"2025-03-27T15:06:57.361902Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 173MB/s] \n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:07:13.672502Z","iopub.execute_input":"2025-03-27T15:07:13.672870Z","iopub.status.idle":"2025-03-27T15:07:13.677646Z","shell.execute_reply.started":"2025-03-27T15:07:13.672846Z","shell.execute_reply":"2025-03-27T15:07:13.676835Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**testing certieria**","metadata":{}},{"cell_type":"code","source":"\nclass_weights = class_weights / class_weights.sum()  # Normalize to sum to 1\n\nclass_weights = class_weights.to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:07:16.800500Z","iopub.execute_input":"2025-03-27T15:07:16.800896Z","iopub.status.idle":"2025-03-27T15:07:16.806430Z","shell.execute_reply.started":"2025-03-27T15:07:16.800855Z","shell.execute_reply":"2025-03-27T15:07:16.805655Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"num_epochs = 50\n\npatience = 40\nbest_metric = -np.inf  \n\nepochs_no_improve = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:07:18.966683Z","iopub.execute_input":"2025-03-27T15:07:18.967041Z","iopub.status.idle":"2025-03-27T15:07:18.971283Z","shell.execute_reply.started":"2025-03-27T15:07:18.967016Z","shell.execute_reply":"2025-03-27T15:07:18.970276Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train_statistics():\n    \n    all_preds, all_labels, all_probs = [], [], []\n    \n    with torch.no_grad():\n        \n        for images, labels in train_loader:\n            \n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs)\n\n   \n    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    mean_auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\", average=\"macro\")\n    \n    print(f\" for train: Epoch  Balanced Acc: {balanced_acc:.4f}, Macro F1: {macro_f1:.4f}, Mean AUC: {mean_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:07:22.192647Z","iopub.execute_input":"2025-03-27T15:07:22.192988Z","iopub.status.idle":"2025-03-27T15:07:22.199284Z","shell.execute_reply.started":"2025-03-27T15:07:22.192960Z","shell.execute_reply":"2025-03-27T15:07:22.198327Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"last = False\nfor epoch in range(num_epochs):\n    \n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in train_loader:\n        \n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n\n        probs = torch.softmax(outputs, dim=1)\n        loss = criterion(probs, labels).to(device)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n    scheduler.step()\n\n    model.eval()\n    train_statistics()\n    all_preds, all_labels, all_probs = [], [], []\n\n    with torch.no_grad():\n        \n        for images, labels in valid_loader:\n            \n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs)\n\n   \n    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    mean_auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\", average=\"macro\")\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n          f\"Balanced Acc: {balanced_acc:.4f}, Macro F1: {macro_f1:.4f}, Mean AUC: {mean_auc:.4f}\")\n\n  \n    if mean_auc > best_metric:\n        \n        best_metric = mean_auc\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), \"best_model.pth\")  \n        last = True\n        \n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n\nif last == False:\n    torch.save(model.state_dict(), \"best_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:07:29.879298Z","iopub.execute_input":"2025-03-27T15:07:29.879622Z","execution_failed":"2025-03-28T12:45:56.195Z"}},"outputs":[{"name":"stdout","text":" for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1014, Mean AUC: 0.5081\nEpoch [1/50], Loss: 1.3404, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.5696\nEpoch [1/50], Loss: 1.3404\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0995, Mean AUC: 0.5256\nEpoch [2/50], Loss: 1.3020, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.4722\nEpoch [2/50], Loss: 1.3020\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0997, Mean AUC: 0.6453\nEpoch [3/50], Loss: 1.2702, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.4752\nEpoch [3/50], Loss: 1.2702\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0997, Mean AUC: 0.5050\nEpoch [4/50], Loss: 1.2691, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.5076\nEpoch [4/50], Loss: 1.2691\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1001, Mean AUC: 0.4907\nEpoch [5/50], Loss: 1.2678, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.6598\nEpoch [5/50], Loss: 1.2678\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0999, Mean AUC: 0.4978\nEpoch [6/50], Loss: 1.2681, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.6711\nEpoch [6/50], Loss: 1.2681\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1008, Mean AUC: 0.4919\nEpoch [7/50], Loss: 1.2646, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.6366\nEpoch [7/50], Loss: 1.2646\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0992, Mean AUC: 0.5587\nEpoch [8/50], Loss: 1.2666, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.4933\nEpoch [8/50], Loss: 1.2666\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0994, Mean AUC: 0.5068\nEpoch [9/50], Loss: 1.2663, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4289\nEpoch [9/50], Loss: 1.2663\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0993, Mean AUC: 0.4937\nEpoch [10/50], Loss: 1.2687, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.4208\nEpoch [10/50], Loss: 1.2687\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1007, Mean AUC: 0.4977\nEpoch [11/50], Loss: 1.2651, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.4435\nEpoch [11/50], Loss: 1.2651\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0997, Mean AUC: 0.5011\nEpoch [12/50], Loss: 1.2679, Balanced Acc: 0.2500, Macro F1: 0.0167, Mean AUC: 0.4450\nEpoch [12/50], Loss: 1.2679\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1001, Mean AUC: 0.4969\nEpoch [13/50], Loss: 1.2650, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4398\nEpoch [13/50], Loss: 1.2650\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1003, Mean AUC: 0.4963\nEpoch [14/50], Loss: 1.2694, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4380\nEpoch [14/50], Loss: 1.2694\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0994, Mean AUC: 0.4983\nEpoch [15/50], Loss: 1.2695, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4640\nEpoch [15/50], Loss: 1.2695\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0995, Mean AUC: 0.4963\nEpoch [16/50], Loss: 1.2693, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4368\nEpoch [16/50], Loss: 1.2693\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1011, Mean AUC: 0.4945\nEpoch [17/50], Loss: 1.2679, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4677\nEpoch [17/50], Loss: 1.2679\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0986, Mean AUC: 0.4935\nEpoch [18/50], Loss: 1.2690, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4496\nEpoch [18/50], Loss: 1.2690\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0992, Mean AUC: 0.4991\nEpoch [19/50], Loss: 1.2664, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4396\nEpoch [19/50], Loss: 1.2664\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1007, Mean AUC: 0.4991\nEpoch [20/50], Loss: 1.2669, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4598\nEpoch [20/50], Loss: 1.2669\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1002, Mean AUC: 0.4982\nEpoch [21/50], Loss: 1.2672, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4406\nEpoch [21/50], Loss: 1.2672\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1008, Mean AUC: 0.4985\nEpoch [22/50], Loss: 1.2664, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4304\nEpoch [22/50], Loss: 1.2664\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1002, Mean AUC: 0.4998\nEpoch [23/50], Loss: 1.2654, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4305\nEpoch [23/50], Loss: 1.2654\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1003, Mean AUC: 0.4937\nEpoch [24/50], Loss: 1.2701, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4331\nEpoch [24/50], Loss: 1.2701\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0991, Mean AUC: 0.4945\nEpoch [25/50], Loss: 1.2665, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4524\nEpoch [25/50], Loss: 1.2665\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1001, Mean AUC: 0.4992\nEpoch [26/50], Loss: 1.2674, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4277\nEpoch [26/50], Loss: 1.2674\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1002, Mean AUC: 0.4974\nEpoch [27/50], Loss: 1.2663, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4640\nEpoch [27/50], Loss: 1.2663\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0995, Mean AUC: 0.4951\nEpoch [28/50], Loss: 1.2672, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4562\nEpoch [28/50], Loss: 1.2672\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1003, Mean AUC: 0.4968\nEpoch [29/50], Loss: 1.2665, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4293\nEpoch [29/50], Loss: 1.2665\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0988, Mean AUC: 0.4976\nEpoch [30/50], Loss: 1.2685, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4497\nEpoch [30/50], Loss: 1.2685\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1010, Mean AUC: 0.4973\nEpoch [31/50], Loss: 1.2666, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4167\nEpoch [31/50], Loss: 1.2666\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0999, Mean AUC: 0.4973\nEpoch [32/50], Loss: 1.2656, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4470\nEpoch [32/50], Loss: 1.2656\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0989, Mean AUC: 0.4979\nEpoch [33/50], Loss: 1.2674, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4264\nEpoch [33/50], Loss: 1.2674\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1001, Mean AUC: 0.4990\nEpoch [34/50], Loss: 1.2665, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4402\nEpoch [34/50], Loss: 1.2665\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1002, Mean AUC: 0.4968\nEpoch [35/50], Loss: 1.2674, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4352\nEpoch [35/50], Loss: 1.2674\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1018, Mean AUC: 0.4995\nEpoch [36/50], Loss: 1.2666, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4362\nEpoch [36/50], Loss: 1.2666\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1004, Mean AUC: 0.4975\nEpoch [37/50], Loss: 1.2674, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4408\nEpoch [37/50], Loss: 1.2674\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1007, Mean AUC: 0.4970\nEpoch [38/50], Loss: 1.2660, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4636\nEpoch [38/50], Loss: 1.2660\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.1001, Mean AUC: 0.4986\nEpoch [39/50], Loss: 1.2686, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4328\nEpoch [39/50], Loss: 1.2686\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0995, Mean AUC: 0.4970\nEpoch [40/50], Loss: 1.2669, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4587\nEpoch [40/50], Loss: 1.2669\n for train: Epoch  Balanced Acc: 0.2500, Macro F1: 0.0996, Mean AUC: 0.4950\nEpoch [41/50], Loss: 1.2686, Balanced Acc: 0.2500, Macro F1: 0.0166, Mean AUC: 0.4595\nEpoch [41/50], Loss: 1.2686\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}